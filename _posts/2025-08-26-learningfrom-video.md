---
title: "Learning from Video 笔记"
date: 2025-08-26 10:00:00 +0800
categories: [Others]
layout: single
author_profile: true
---

# Any-point Trajectory Modeling for Policy Learning
概括地说：中层规划器

利用互联网数据
研究如何从视频中提取经验知识

视频生成困难
不想做视频预测。未来的视频图像长什么样，对于机器人控制没有任何意义。
我们只需要知道每个物体的移动方向，而不在乎画面的精美程度。

在游戏或仿真引擎中，他们不会把某一个物体建模成一个对象，
而是把物体中的每一个粒子建模成一个对象，通过建模粒子之间的碰撞、挤压和相对移动来对整个场景进行建模。

通过时间的展开，你可以看到袋子上点的运动和机械臂上点的运动是不一样的，但相同物体上的点的运动趋势又是相似的。



## 网络结构
![]<640.png>
输入视频数据
使用之前提到的跟踪模型（tracking model）来获取每个视频帧上每个点的未来轨迹

当我们知道每个点的轨迹后，我们可以训练一个我们称之为“track transformer”的模型。
这个模型基于transformer，是一种多模态的模型。它的输入包括当前的图片、语言描述和要查询的点的坐标。训练目标是预测这些查询点在未来的轨迹。

预测的轨迹将作为输入提供给我们的控制模型。这个控制模型不仅以图片作为输入，还包括预测的轨迹，然后输出相应的动作。


### 2D ViT

## Related Works
UniPi 直接使用视频预测
这种方法通过视频学习，比如给定一个任务如倒水，给定第一帧和“倒水”这个任务，
模型会预测在倒水过程中未来视频的变化。预测出来这些视频之后，再通过视频反推出来机械臂的动作，从而执行这些动作。
这种方法的优点是他非常的端到端，因为直接拿视频过来去训练视频预测模型，视频预测模型出来的东西去训练恢复机器人动作的inverse dynamics model。
这种方法的优势在于可以使用大量数据，但问题是计算量很大，因为需要先生成视频，可能一段三到五秒的视频就需要十几分钟到半个小时，甚至更久。