---
title: "深度学习笔记"
date: 2025-08-23 10:00:00 +0800
categories: [深度学习]
layout: single
author_profile: true
---

# DL

## 目录

- [1. 基础知识](#1-基础知识)

  - [1.1 机器学习（ML）](#11-机器学习ml)

    - [1.1.1 三大类型](#111-三大类型)

      - [① 监督学习](#监督学习supervised-learning)

      - [② 无监督学习](#无监督学习unsupervised-learning)

      - [③ 强化学习](#强化学习reinforcement-learning)

    - [1.1.2 常见算法](#112-常见算法)

  - [1.2. 深度学习（DL）](#2-深度学习dl)

- [3. 重要术语速查](#7-重要术语速查)

- [4. 相关论文](#8-相关论文)

---

## 1. 基础知识

### 1.1 机器学习（ML）

### 1.1.1 三大类型

---

### 监督学习（Supervised Learning）

- **定义**：用带标签的数据训练模型，学习输入与输出之间的映射关系。  
- **常见任务**：
  - 分类（Classification）：输出离散类别，如猫/狗识别。
  - 回归（Regression）：输出连续值，如房价预测。

### 无监督学习(Unsupervised Learning)

- **定义**：用无标签数据寻找结构或模式。
- **常见任务**：
  - 聚类（Clustering）：如 K-means，将数据分成簇。
  - 降维（Dimensionality Reduction）：如 PCA，压缩特征空间。
- **挑战**：
  - 如何发现数据的潜在结构。
  - 如何在无标签条件下评估模型好坏。

### 强化学习（Reinforcement Learning）

[强化学习笔记](./rl.md)

### 1.1.2 常见算法

#### 1. 线性回归  

- 线性回归就是用一条（或一个超平面）拟合数据，使预测值与真实值的误差最小。

- 模型公式（多元形式）

  $$
  \hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n
  $$

  其中：
  - $\hat{y}$：预测值  
  - $w_0$：截距项（bias）  
  - $w_1, w_2, \dots, w_n$：特征的回归系数  
  - $x_1, x_2, \dots, x_n$：输入特征

  目标是最小化均方误差（MSE）：

  $$
  \text{MSE} = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2
  $$

  其中：
  - $m$：样本数量  
  - $y_i$：第 $i$ 个样本的真实值

- 核心思想：找到最佳的 \(w\) 让预测线最贴近所有数据点。

#### 2. KNN（K-Nearest Neighbors）  

- 步骤：
  1. 计算待预测样本与训练集中每个样本的距离（常用欧几里得距离、曼哈顿距离等）。
  1. 按距离从小到大排序，选取前 K 个邻居。
  1. 分类：多数表决法（投票）；回归：取邻居标签的均值（或加权均值）。

- **优点**：
  - 简单直观、无需训练过程。
  - 对多类别任务自然支持。

- **缺点**：
  - 预测开销大（需存储全部训练样本）。
  - 对高维数据敏感（维度灾难）。
  - 特征量纲影响大（需归一化/标准化）。

- **适用场景**：
  - 数据量较小、特征数适中。
  - 决策边界复杂、难用简单方程表示的情况。

#### 3. 逻辑回归

- Sigmoid 函数，将输出映射到 (0,1) 概率空间，构造分类决策边界。

- 二分类问题

- 损失函数用交叉熵

#### 4. 决策树（Decision Tree）

- **定义**：基于特征划分数据形成树状结构，通过一系列 **if-else** 规则进行分类或回归。
- **原理**：
  1. 从根节点开始，根据某个特征的分裂规则将数据划分成更纯净的子集。
  1. 重复递归分裂直到满足停止条件（如最大深度、最小样本数）。
  1. 预测时沿着路径走到叶节点，输出类别或数值。

- **划分标准**：
  - 分类任务：信息增益（ID3）、信息增益率（C4.5）、基尼指数（CART）。
  - 回归任务：均方误差（MSE）最小化。

- **优点**：
  - 可解释性强（规则可视化）。
  - 无需特征缩放。
  - 能处理数值型和类别型特征。

- **缺点**：
  - 易过拟合（尤其是深树）。
  - 对数据微小波动敏感（不稳定）。
  - 贪心划分可能陷入局部最优。

- **适用场景**：
  - 特征与标签存在明显规则关系。
  - 需要可解释模型的业务场景。

#### 5. 随机森林（Random Forest）

- **定义**：集成学习的一种 Bagging 方法，通过训练多个决策树并投票/平均来提升性能。
- **原理**：
  1. 样本随机性：每棵树用 Bootstrap（有放回抽样）训练子集。
  1. 特征随机性：节点分裂时随机选部分特征寻找最佳划分。
  1. 预测时：分类用多数表决，回归取平均值。

- **超参数**：
  - 树的数量（n_estimators）
  - 最大深度（max_depth）
  - 每次划分考虑的特征数（max_features）
  - 最小样本分裂数（min_samples_split）

- **优点**：
  - 减少过拟合（相对单棵树）。
  - 对缺失值与异常值鲁棒。
  - 特征重要性可解释。

- **缺点**：
  - 可解释性不如单棵树。
  - 预测速度比单棵树慢（但可并行）。

- **适用场景**：
  - 大部分分类/回归任务的高性能基线。
  - 特征多、关系复杂、非线性强的数据集。

#### 6. 支持向量机（SVM, Support Vector Machine）

- **核心思想**

  - SVM 的目标是找到一个**能够最大化分类间隔（Margin）的超平面**，这样模型在新数据上的泛化能力最好。

  - 不是随便画一条能分开两类的直线，而是要**离两类数据都尽量远**的那条线。
  - 只需要关心那些**离边界最近的样本**（支持向量），其他样本不直接影响超平面的位置。

---

- **数学定义（线性可分情况）**

  假设数据集为：

  $$
  (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m), \quad y_i \in \{-1, +1\}
  $$

  目标是找到一个超平面：

  $$
  w \cdot x + b = 0
  $$

  使得：

  $$
  y_i(w \cdot x_i + b) \ge 1
  $$

  并且让间隔（margin）最大化。

  **分类间隔公式**：

  $$
  \text{Margin} = \frac{2}{\|w\|}
  $$

  所以等价的优化问题是：

  $$
  \min_{w,b} \frac{1}{2} \|w\|^2
  $$

  约束条件：

  $$
  y_i (w \cdot x_i + b) \ge 1, \quad i=1,\dots,m
  $$

---

- **线性不可分与软间隔**

  如果数据不能完美分开（线性不可分），SVM 会允许部分样本违反间隔约束，引入**松弛变量** \\( \xi_i \\) 和**惩罚系数** \\( C \\)：

  优化目标：

  $$
  \min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^m \xi_i
  $$

  约束：

  $$
  y_i(w \cdot x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
  $$

  - \\( C \\) 越大 → 罚错得越重 → 更接近硬间隔。
  - \\( C \\) 越小 → 容忍更多分类错误 → 间隔更大。

  ---

- **非线性可分与核函数**

  SVM 可以通过**核函数（Kernel Trick）**把数据映射到高维空间，使其线性可分。

  常见核函数：

  - 线性核（Linear）
  - 多项式核（Polynomial）
  - 高斯径向基核（RBF）
  - Sigmoid 核

  核技巧的关键是：

  不用显式计算高维映射，只需要计算内积：

  $$
  K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
  $$

  ---

- **决策函数**

  训练完成后，SVM 的预测公式为：

  $$
  f(x) = \text{sign}\left( \sum_{i \in SV} \alpha_i y_i K(x_i, x) + b \right)
  $$

  其中：

  - \\( SV \\)：支持向量的索引集合
  - \\( \alpha_i \\)：拉格朗日乘子
  - \\( b \\)：偏置项

  ---

- **训练过程概览**

  1. **准备数据**：特征归一化（对 SVM 收敛速度很重要）。
  2. **选择核函数**（线性 / RBF / 多项式等）。
  3. **构造优化问题**（二次规划 QP）。
  4. **求解拉格朗日对偶问题**：

     $$
     \max_{\alpha} \sum_i \alpha_i - \frac12 \sum_{i,j} \alpha_i \alpha_j y_i y_j K(x_i, x_j)
     $$

     约束：

     $$
     0 \le \alpha_i \le C, \quad \sum_i \alpha_i y_i = 0
     $$

  5. **得到支持向量**（\\( \alpha_i > 0 \\) 的样本）。
  6. **计算 b**。
  7. **预测新样本**：用决策函数判断类别。

  ---

- **SVM 的优缺点**

  **优点**：

  - 对高维数据表现好。
  - 核函数可处理非线性问题。
  - 只依赖支持向量，内存效率高。

  **缺点**：

  - 大数据集训练慢。
  - 参数（C、γ）敏感。
  - 对噪声和重叠类敏感。

#### 7. 朴素贝叶斯（Naive Bayes）

- 条件概率：在某个事件已发生条件下另一个事件的概率。
- 先验概率：无条件信息下事件发生的概率。
- 后验概率：结合新证据更新后的事件概率。

  ---

## 2. 深度学习（DL）

- 基本概念：神经元，权重，偏差，激活函数，输入，输出，隐藏层

### 全连接神经网络

- **定义**：  
  全连接神经网络是一种基础的神经网络结构，相邻两层之间的每一个神经元都与下一层的所有神经元相连接。它是多层感知机（MLP）的核心组成部分。

- **结构特点**：  
  - 输入层（Input Layer）：接收原始特征向量。  
  - 隐藏层（Hidden Layers）：每个神经元与前一层的所有神经元相连，并通过激活函数引入非线性。  
  - 输出层（Output Layer）：根据任务输出分类结果或回归值。  

- **数学表达**：  
  对于第 $l$ 层：
  $$
  z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
  $$
  $$
  a^{(l)} = f(z^{(l)})
  $$
  其中：  
  - $W^{(l)}$：权重矩阵  
  - $b^{(l)}$：偏置向量  
  - $a^{(l-1)}$：上一层的输出  
  - $f(\cdot)$：激活函数（如 ReLU、Sigmoid、Tanh）  

- **特点与应用**：  
  - 能够拟合复杂的非线性关系。  
  - 对输入数据的空间结构没有利用（比如图像的空间信息），因此在图像/序列任务中通常和 CNN、RNN 等结合使用。  
  - 在分类、回归、特征提取等任务中广泛应用。  

- **优点**：  
  - 结构简单，通用性强。  
  - 理论上只要隐藏层足够多且神经元数量足够，能逼近任意连续函数（通用逼近定理）。  

- **缺点**：  
  - 参数量大，容易过拟合。  
  - 不利用数据的局部特征，计算效率较低。  
  - 对高维输入（如大图像）训练代价高。

### 卷积神经网络(CNN, Convolutional Neural Network)

- **定义**：  
  卷积神经网络是一种专门用于处理具有网格拓扑结构数据（如图像、语音）的神经网络，通过卷积操作提取局部特征，并利用权重共享和局部连接减少参数数量。

- **结构特点**：  
  - 卷积层（Convolutional Layer）：使用卷积核（filter）在输入上滑动并计算局部加权和，提取局部特征。  
  - 激活函数（Activation Function）：引入非线性，例如 ReLU、Leaky ReLU、GELU 等。  
  - 池化层（Pooling Layer）：下采样特征图，减少特征维度和计算量（常用 Max Pooling、Average Pooling）。  
  - 全连接层（Fully Connected Layer）：将卷积特征映射到输出空间（如分类概率）。  
  - 输出层（Output Layer）：根据任务输出最终结果。

- **数学表达**：  
  对于单通道 2D 卷积：
  $$
  (X * K)(i, j) = \sum_m \sum_n X(i+m, j+n) \cdot K(m, n)
  $$
  其中：  
  - $X$：输入特征图  
  - $K$：卷积核  
  - $(i, j)$：卷积核左上角对应输入的位置索引  

- **关键概念**：  
  - **局部感受野（Local Receptive Field）**：每个卷积核只感受输入的一小部分区域。  
  - **权重共享（Weight Sharing）**：同一卷积核在不同位置使用相同参数，减少参数量。  
  - **步幅（Stride）**：卷积核移动的像素步长，影响输出大小。  
  - **填充（Padding）**：在输入边界补零以控制输出尺寸。  

- **优点**：  
  - 参数少，计算效率高。  
  - 能有效提取空间层级特征（低层检测边缘，高层检测复杂形状）。  
  - 对平移、缩放等具有一定不变性。  

- **缺点**：  
  - 对旋转、仿射变化敏感。  
  - 对大范围依赖建模能力有限（可结合注意力机制改进）。  

- **应用场景**：  
  - 图像分类、目标检测、语义分割。  
  - 视频分析、语音识别。  
  - 医学影像、遥感图像处理等。

## 循环神经网络（RNN, Recurrent Neural Network）

- **核心思想**：利用隐藏状态 $h_t$ 保存历史信息，适合处理序列数据。

- **输入**：预训练词向量（比如 word2vec、GloVe、FastText、BERT 的 embedding 层）/训练
- **状态更新公式**：

  $$
  h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
  $$
  $$
  y_t = g(W_{hy}h_t + b_y)
  $$

  其中 $f$ 和 $g$ 为非线性函数（如 tanh、softmax）。
W：权重矩阵把当前输入向量 𝑥𝑡xt映射到隐藏状态的空间

- **缺点**：
  - 容易出现梯度消失/爆炸。
  - 难以捕捉长距离依赖。

---

## **长短期记忆网络（LSTM）与门控循环单元（GRU）**

- **LSTM 核心结构**：
  - **遗忘门**：
    $$
    f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
    $$
  - **输入门**：
    $$
    i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
    $$
    $$
    \tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)
    $$
  - **细胞状态更新**：
    $$
    C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
    $$
  - **输出门**：
    $$
    o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
    $$
    $$
    h_t = o_t \cdot \tanh(C_t)
    $$
- **GRU 简化**：
  - 只有更新门和重置门，计算更快。

---

## 7. 重要术语速查

- **Stacked Layers**：多层相同/相似结构顺序堆叠。
- **Vanishing Gradient**：梯度在反向传播中指数衰减。
- **Exploding Gradient**：梯度在反向传播中指数放大。
- **Residual Connection**：通过捷径连接缓解深度训练困难。
- **identity mapping**：恒等映射
- **FLOPS**：floating point operations per second
- **Params**：参数的数量。
- **BN**：Batch Nomalization,使各层输入分布更稳定

---

## 8. 相关论文

### Alexnet

[ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

- **基本结构**：输入为RGB三通道的224 × 224 × 3大小的图像,最终输出层为softmax，将网络输出转化为概率值，用于预测图像的类别。

- **创新点**：深度卷积神经网络，ReLU，数据增强，Dropout，两个GPU分布式训练

### Resnet

[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

- **要解决的问题**：深度网络中梯度消失或爆炸 显示构造出恒等映射

- **核心思想（残差学习）**：
  - 让网络学习 **残差映射** 而不是直接学习期望映射：
    $$
    y = F(x) + x
    $$
    其中：
    - $x$：输入  
    - $F(x)$：残差函数（由若干卷积层组成）  
    - $y$：输出
  - 如果最优映射接近恒等映射（identity mapping），只需 $F(x) \approx 0$ 即可，大大减轻优化难度。

- **基本结构**

  - **输入部分（Stem）**：一般是一个 7×7 卷积层 + 最大池化层，负责初步提取低层特征

  - **主干网络**：由多个 Basic Block 或 Bottleneck Block 顺序堆叠而成，每个 Stage 的第一个 Block 可能会做 降采样（stride=2）以缩小特征图大小、增大感受野。Block 内部通过 残差连接（skip connection） 解决梯度消失/退化问题。

  - **输出**：全局平均池化（GAP）层+全连接层（分类器）输出最终预测结果

- **网络结构举例（ResNet-18）**

  - Conv1：$7 \times 7$ 卷积，stride=2  
  - MaxPool：$3 \times 3$，stride=2  
  - Conv2_x：Basic Block × 2  
  - Conv3_x：Basic Block × 2（通道数翻  倍）  
  - Conv4_x：Basic Block × 2（通道数翻倍）  
  - Conv5_x：Basic Block × 2（通道数翻倍）  
  - Global Average Pooling  
  - 全连接层（FC）输出分类结果

> 注：ResNet50、ResNet101、ResNet152中Basic Block优化为Bottleneck Block：1×1卷积核降维，再升维 (256d->64-d->256-d)，减少计算量(FLOPS)


