---
title: "强化学习"
date: 2025-08-22 10:00:00 +0800
categories: [强化学习]
layout: single
author_profile: true
---

# RL

## 基础知识

### 什么是强化学习？

智能体通过与环境交互，不断学习，完成特定目标

### 基本元素

状态 $s$ (state)

动作 $a$ (action)

策略 $\pi(a|s) = P(A_t=a | S_t=s)$

在策略π和状态s时，采取行动后的价值（value），用$v_π(s)$表示

奖励(reward) : $t$ 时刻智能体在状态 $S_t$ , 采取的动作为 $A_t$ , 对应的奖励 $R_t+1$ 会在 $t+1$ 时刻得到

环境的状态转化模型，在状态 $s$ 下采取动作 $a$ ,转到下一个状态 $s′$ 的概率，$P_{s,s'}^a$

一句话来讲：在某个 $state$ 下，$agent$ 依据 $policy$，采取$action$ ，与$environment$ 交互，$agent$ 获得反馈 $reward$。$agent$ 获得的 $reward$ 会指导 $policy$ 改进，在 $state$ 选择 $action$。循环往复，$policy$ 不断被优化

### 马尔可夫性质

马尔可夫性质是指在给定当前状态的情况下，未来状态的条件概率分布仅依赖于当前状态，而与过去状态无关。

$ P(S_(t+1)|S_t)=P(S_t+1|S_t,S_(t-1),...S_1)$

### 状态价值与动作价值

#### 状态价值

$v_{\pi}(s) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s) $

#### 动作价值

$q_{\pi}(s,a) = \mathbb{E}_{\pi}(G_t|S_t=s, A_t=a) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s,A_t=a)$

#### 转换关系

根据动作价值函数 $q_{\pi}(s,a)$ 和状态价值函数 $v_{\pi}(s)$ 的定义，我们很容易得到他们之间的转化关系 $v_{\pi}(s) = \sum\limits_{a \in A} \pi(a|s)q_{\pi}(s,a)$

> 状态价值函数是所有动作价值函数基于策略 $π$ 的期望。

### Model-Based 与 Model-Free

- Model-Based 意味着有环境的 **explicit model**，即 $P(S_{t+1},R|S_t ,a_t) $

- Model-Free 相对应地，无

### On-policy 与 Off-policy

- the learning is from the data off the target policy

### DP (Model-Based)

#### Policy Iteration

给定一个初始策略 $π$ ，可以得到基于该策略的价值函数 $v_π$ (用贝尔曼方程计算)

基于该价值函数又可以得到一个贪婪策略 ${\pi}^′$ = $ greedy(v_π)$

如此反复进行，价值函数和策略均得到迭代更新，并最终收敛得到最优价值函数 $v^∗$ 和最优策略 $π^∗$

#### Value Iteration

直接迭代更新最优价值函数 $V^*$（用贝尔曼最优方程），无需显式维护策略，最终从收敛的 $V^*$ 中导出最优策略 $\pi^*$

### 蒙特卡洛法 (Model-Free)

不基于模型
通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。

所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。

要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解预测：

$$
G_t =R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+...  \gamma^{T-t-1}R_{T}
$$

$$
Average(G_t),  s.t. S_t=S
$$

控制：ϵ−贪婪法 使用1−ϵ的概率贪婪地选择目前认为是最大行为价值的行为，而用ϵ的概率随机的从所有可选行为中选择行为

### 时序差分方法(TD)

TD(0)：一步更新

$$
V(s_t) \leftarrow V(s_t) + \alpha \Big( r_{t+1} + \gamma\ V(s_t+1)-V(s_t)\Big)
$$

我们容易想到，TD(1)，TD(2)也是可取的

### Q-learning

#### 核心思想

- 学习 **动作价值函数 $Q(s,a)$**：在状态 $s$ 下执行动作 $a$，并按最优策略走，能获得的期望回报。
- 最优 Q 函数满足 **Bellman 最优方程**：

$$
Q^*(s,a) = \mathbb{E}\Big[ r + \gamma \max_{a'} Q^*(s',a') \Big]
$$

#### 更新公式

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \Big( r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big)
$$

其中：

- $\alpha$：学习率  
- $r$：即时奖励  
- $\gamma$：折扣因子  
- $\max_{a'} Q(s',a')$：下一状态的最佳 Q 值估计  

#### 对公式的直观理解

- **旧值**：原有对 $Q(s,a)$ 的估计  
- **目标值**：$r + \gamma \max_{a'} Q(s',a')$  
- **更新**：往目标值方向移动一点，幅度由 $\alpha$ 决定  

## 从强化学习到深度强化学习

### 为什么我们需要神经网络？

解决高维的问题

Atari游戏，机器人控制等问题中，参数量巨大，维护一个Q表格是不现实的

### DQN 

#### 

$$
Q(s,a;\theta) \approx Q^*(s,a)
$$

#### 核心机制
1. **经验回放（Replay Buffer）**
   - 存储 transition信息 $(s, a, r, s′)$，训练时随机采样，打破数据相关性 
   

> 用相邻的样本连续训练evaluate network会带来网络过拟合泛化能力差的问题

2. **目标网络（Target Network）**
   - 复制一份 Q 网络作为目标网络 $\theta^-$，周期性更新  
   - 目标值：

$$
y = r + \gamma \max_{a'} Q(s',a'; \theta^-)
$$

3. **损失函数**
$$
L(\theta) = \Big( y - Q(s,a;\theta) \Big)^2
$$ 



#### 训练流程
1. 初始化 Q 网络和目标网络  
2. 建立经验回放池  
3. 每步：
   - $\epsilon$-贪心选择动作  
   - 执行动作，存储 $(s,a,r,s′)$  
   - 从回放池采样 batch，计算目标值 $y$  
   - 更新 Q 网络参数 $\theta$  
   - 每隔 $N$ 步更新目标网络  
4. 循环直到收敛  

#### 注意
evaluate network 用来计算策略选择的Q值 和 Q值迭代更新，梯度下降、反向传播的是evaluate network

target network用来计算TD中下一状态的Q值，网络参数更新来自evaluate network网络参数复制（保持目标值稳定，防止过拟合）


### Actor-Critic框架
- Actor（执行者）：负责 “做决策”，输出动作策略。它根据当前环境状态，直接输出具体动作（确定性策略）或动作的概率分布（随机性策略），决定智能体下一步该做什么。

- Critic（评论家）：负责 “评好坏”，评估动作价值。它根据当前状态和 Actor 选择的动作，计算该动作带来的 “价值”（通常是 Q 值或状态价值 V），判断这个决策的长期收益高低。

双向优化：

- 优化 Actor：用 Critic 给出的 TD Error 作为反馈信号，调整 Actor 的参数，让它未来更倾向于选择 Critic 评分高的动作。

- 优化 Critic：根据 TD Error 调整自身参数，减少 “预期价值” 与 “实际价值” 的差距，提升评估的准确性。

```python
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

# 超参数
GAMMA = 0.99
LR = 1e-3
HIDDEN_SIZE = 128
MAX_EPISODES = 5000

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Actor-Critic 网络
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # 公共层
        self.fc1 = nn.Linear(state_dim, HIDDEN_SIZE)
        # Actor 分支：输出每个动作的概率
        self.actor = nn.Linear(HIDDEN_SIZE, action_dim)
        # Critic 分支：输出状态价值 V(s)
        self.critic = nn.Linear(HIDDEN_SIZE, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        logits = self.actor(x)
        probs = F.softmax(logits, dim=-1)  # 动作概率分布
        value = self.critic(x)             # 状态价值 V(s)
        return probs, value


def train():
    env = gym.make("CartPole-v1")

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    model = ActorCritic(state_dim, action_dim).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    for episode in range(MAX_EPISODES):
        state, _ = env.reset()
        done = False
        log_probs = []
        values = []
        rewards = []

        # 采样一条轨迹
        while not done:
            state_tensor = torch.tensor([state], dtype=torch.float32, device=DEVICE)
            probs, value = model(state_tensor)

            # 从分布中采样动作
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()

            # 保存对数概率、价值
            log_probs.append(dist.log_prob(action))
            values.append(value)

            # 与环境交互
            next_state, reward, done, truncated, _ = env.step(action.item())
            done = done or truncated

            rewards.append(reward)
            state = next_state

        # 计算返回 Gt（蒙特卡洛）
        # returns = []
        # G = 0
        # for r in reversed(rewards):
        #     G = r + GAMMA * G
        #     returns.insert(0, G)
        # returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)

        # 转为张量
        log_probs = torch.stack(log_probs)
        values = torch.cat(values).squeeze()

        # 一步TD目标：target_t = r_t + γ * V(s_{t+1})
        with torch.no_grad():
            rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)  # [T]
            next_values = torch.cat([values[1:], torch.zeros(1, device=DEVICE)])   # 末步的 V(s_{T+1})=0
            td_targets = rewards_t + GAMMA * next_values                           # [T]

        # Advantage: A_t = target_t - V(s_t)
        advantages = td_targets - values


        # 损失函数：Actor Loss + Critic Loss
        actor_loss = -(log_probs * advantages.detach()).mean()
        critic_loss = advantages.pow(2).mean()
        loss = actor_loss + critic_loss

        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # 打印结果
        if (episode + 1) % 10 == 0:
            total_reward = sum(rewards)
            print(f"Episode {episode+1}: return={total_reward}, loss={loss.item():.3f}")
            
    env.close()


if __name__ == "__main__":
    train()
```


### SAC
最大熵
### SAC（Soft Actor-Critic）

#### 核心目标
$$
\pi^*(a|s) = \arg\max_\pi \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t \left( r_t + \alpha H(\pi(\cdot|s_t)) \right) \right]
$$
- 最大化累积奖励的同时，最大化策略的熵 $H(\pi(\cdot|s)) = -\mathbb{E}_{a\sim\pi}[ \log \pi(a|s) ]$（鼓励探索）
- $\alpha$ 为熵权重，平衡奖励与探索


#### 核心机制
1. **随机策略（Stochastic Policy）**
   - Actor 网络输出动作的概率分布（连续空间常用高斯分布）：$\pi(a|s;\phi)$，通过采样生成动作
   - 相比确定性策略，随机策略天然支持熵最大化，提升探索效率和策略鲁棒性

2. **双 Q 网络（Dual Q-Networks）**
   - 两个独立的 Q 网络 $Q_1(s,a;\theta_1)$、$Q_2(s,a;\theta_2)$，评估状态-动作价值（含熵奖励）
   - 抑制价值估计过高的偏差（类似 DQN 目标网络的改进，但更激进）

3. **目标 Q 网络（Target Q-Networks）**
   - 对应双 Q 网络的目标网络 $Q_1^-(s,a;\theta_1^-)$、$Q_2^-(s,a;\theta_2^-)$
   - 目标值计算（含熵项）：
   $$
   y = r + \gamma \left( \min_{i=1,2} Q_i^-(s',a';\theta_i^-) - \alpha \log \pi(a'|s';\phi) \right)
   $$
   - 采用指数移动平均（EMA）更新：$\theta_i^- \leftarrow \tau \theta_i + (1-\tau)\theta_i^-$（$\tau \ll 1$，如 0.005），保证目标稳定

4. **经验回放（Replay Buffer）**
   - 存储 transition 信息 $(s,a,r,s',\text{done})$，训练时随机采样，打破数据相关性
   - 支持离线学习（Off-policy），提升样本效率


#### 损失函数
1. **Q 网络损失（Critic 优化）**
   - 最小化当前 Q 值与目标值的均方误差：
   $$
   L(\theta_1) = \mathbb{E}\left[ \left( y - Q_1(s,a;\theta_1) \right)^2 \right]
   $$
   $$
   L(\theta_2) = \mathbb{E}\left[ \left( y - Q_2(s,a;\theta_2) \right)^2 \right]
   $$

2. **策略网络损失（Actor 优化）**
   - 最大化 Q 值与熵的加权和（通过梯度上升实现，等价于最小化负损失）：
   $$
   L(\phi) = -\mathbb{E}_{s\sim D, a\sim\pi}\left[ \min_{i=1,2} Q_i(s,a;\theta_i) - \alpha \log \pi(a|s;\phi) \right]
   $$

3. **熵权重 $\alpha$ 优化（可选）**
   - 自动调整 $\alpha$，使策略熵接近目标值 $H_{\text{target}}$：
   $$
   L(\alpha) = -\mathbb{E}_{s\sim D, a\sim\pi}\left[ \alpha \left( \log \pi(a|s;\phi) + H_{\text{target}} \right) \right]
   $$


#### 训练流程
1. 初始化：
   - Actor 网络 $\pi(\phi)$、双 Q 网络 $Q_1(\theta_1)$、$Q_2(\theta_2)$
   - 目标 Q 网络 $Q_1^-、Q_2^-$（初始参数复制自 $Q_1、Q_2$）
   - 经验回放池 $D$、熵权重 $\alpha$

2. 每步交互：
   - 从当前状态 $s$，用 Actor 网络采样动作 $a \sim \pi(\cdot|s;\phi)$
   - 执行动作，获得奖励 $r$、下一状态 $s'$、终止信号 $\text{done}$
   - 将 $(s,a,r,s',\text{done})$ 存入 $D$

3. 每步训练（从 $D$ 采样批量数据）：
   - 采样 $a' \sim \pi(\cdot|s';\phi)$（带噪声增强探索）
   - 计算目标值 $y$（用目标 Q 网络和 $\log \pi(a'|s')$）
   - 用 $L(\theta_1)、L(\theta_2)$ 更新双 Q 网络
   - 用 $L(\phi)$ 更新 Actor 网络
   - 用 EMA 更新目标 Q 网络参数
   - （可选）用 $L(\alpha)$ 更新熵权重 $\alpha$

4. 循环直到收敛


#### 注意
- Actor 网络负责生成随机策略，通过 Q 网络评估和熵项引导优化
- 双 Q 网络+目标网络抑制价值偏差，经验回放支持离线学习
- 最大熵机制使策略兼具“高奖励”和“高探索性”，在连续动作空间表现优异

### PPO
PPO是一个on-policy的强化学习算法，成为了目前应用最广泛的强化学习算法之一

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.distributions import Normal

class Actor(nn.Module):
   """输出动作分布的均值和方差"""
   def __init__(self, state_dim, action_dim):
      super().__init__()
      self.net = nn.Sequential(
         nn.Linear(state_dim, 64),
         nn.Tanh(),
         nn.Linear(64,64),
         nn.Tanh(),
      )
      self.mu_head = nn.Linear(64, action_dim)
      self.log_std = nn.Parameter(torch.zeros(action_dim))

   def forward(self, state):
      x = self.net(state)
      mu = self.mu_head(x)
      std = self.log_std.exp()
      return mu, std

   def act(self, state):
      mu, std = self.forward(state)
      dist = Normal(mu, std)
      action = dist.sample()
      log_prob = dist.log_prob(action).sum(dim=-1) #多维下所有动作的log概率
      return action, log_prob

class Critic((nn.Module)):
   """估计V(s)"""
   def __init__(self,state_dim):
      super().__init__()
      self.net = nn.Sequential(
         nn.Linear(state_dim,64)
         nn.Linear(64,64),
         nn.Tanh(),
         nn.Linear(64,1)
      )

   def forward(self,state):
      return self.net(state).squeeze(-1)


class PPO:
    def __init__(self, state_dim, action_dim, clip_eps=0.2, gamma=0.99, lam=0.95, lr=3e-4):

        # 两个 Actor：当前 & 旧
        self.actor = Actor(state_dim, action_dim)
        self.actor_old = Actor(state_dim, action_dim)
        self.actor_old.load_state_dict(self.actor.state_dict())

        # 两个 Critic：当前 & target (可选稳定)
        self.critic = Critic(state_dim)
        self.critic_target = Critic(state_dim)
        self.critic_target.load_state_dict(self.critic.state_dict())

        self.opt_actor = optim.Adam(self.actor.parameters(), lr=lr)
        self.opt_critic = optim.Adam(self.critic.parameters(), lr=lr)

        self.gamma, self.lam, self.clip_eps = gamma, lam, clip_eps

   def compute_gae(self, rewards, values, dones):
        """计算 GAE 优势估计"""
        # 优势函数：这个动作比平均值好多少 而Q值难算 用TD改进--> GAE
        # δt​=rt​+γV(st+1​)−V(st​) “实际得到的奖励 + 未来估计” 与 “我之前预测的价值” 的差距。
        adv, gae = [], 0
        next_value = 0
        # [s1, s2, s3, s4] 计算顺序：s4 → s3 → s2 → s1（反向遍历）
        for r, v, done in zip(reversed(rewards), reversed(values), reversed(dones)):
            delta = r + self.gamma * next_value * (1 - done) - v
            gae = delta + self.gamma * self.lam * (1 - done) * gae
            adv.insert(0, gae)
            next_value = v
        return torch.tensor(adv, dtype=torch.float32)

   def update(self, batch):
        """核心PPO更新"""
        states = torch.stack(batch['states'])
        actions = torch.stack(batch['actions'])
        old_logps = torch.stack(batch['logps']).detach()
        rewards = batch['rewards']
        dones = batch['dones']

        with torch.no_grad():
            values = self.critic(states)
            adv = self.compute_gae(rewards, values, dones)
            returns = adv + values
            adv = (adv - adv.mean()) / (adv.std() + 1e-5)  # 标准化

        for _ in range(10):  # K epochs
            mu, std = self.actor(states)
            dist = Normal(mu, std)
            logp = dist.log_prob(actions).sum(-1)
            ratio = torch.exp(logp - old_logps)

            # PPO裁剪目标
            surr1 = ratio * adv
            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv
            loss_actor = -torch.mean(torch.min(surr1, surr2))

            # Critic目标
            value_pred = self.critic(states)
            loss_critic = F.mse_loss(value_pred, returns)

            # 总损失
            loss = loss_actor + 0.5 * loss_critic

            self.opt_actor.zero_grad()
            self.opt_critic.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.opt_actor.step()
            self.opt_critic.step()

        # 同步旧策略
        self.actor_old.load_state_dict(self.actor.state_dict())


# 旧actor网络收集数据
"""
obs = env.reset()
for t in range(T):
    action, logp = agent.actor_old.act(torch.tensor(obs, dtype=torch.float32))
    next_obs, reward, done, _ = env.step(action.numpy())
    buffer.append((obs, action, logp, reward, done))
    obs = next_obs if not done else env.reset()
"""

```
