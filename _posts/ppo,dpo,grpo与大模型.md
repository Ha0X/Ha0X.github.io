---
title: "PPO, DPO, GRPO 与大模型"
date: 2024-01-01 10:00:00 +0800
categories: [强化学习]
layout: single
author_profile: true
---

# 强化学习与大模型

## 为何需要强化学习？

传统的监督微调（Supervised Fine-Tuning, SFT）是训练大模型的第一步。我们用大量高质量的“指令-回答”对来教模型模仿人类的说话方式。然而，SFT有其局限性。首先，高质量的 SFT 数据集构建成本高昂；其次，人类的偏好是复杂且主观的，很难用唯一的“正确答案”来概括。例如，对于 “请写一首关于秋天的诗” 这个指令，存在无数种优秀的回答，SFT很难覆盖所有可能性，也无法告诉模型哪种风格的诗更受欢迎。

为了解决这个问题，研究者们引入了 **从人类反馈中进行强化学习（Reinforcement Learning from Human Feedback, RLHF）** 的框架。

RLHF 的核心思想是，不再直接告诉模型“正确答案是什么”，而是让模型生成一些回答，然后由人类来评判这些回答的好坏（例如，对多个回答进行排序），再利用这些偏好数据来“奖励”或“惩罚”模型，从而引导其生成更符合人类偏好的内容。

这个过程通常分为三个阶段：

1. **监督微调 (SFT)**：使用高质量的标注数据对预训练模型进行初步微调，使其适应特定的指令格式。

2. **奖励模型 (RM) 训练**：让 SFT 模型对同一个指令生成多个不同的回答。人类标注者对这些回答进行排序，形成偏好数据（例如，回答 A > 回答 B）。然后，用这些偏好数据训练一个奖励模型，该模型能够对任意一个“指令-回答”对打分，分数高低代表了人类的偏好程度。

3. **强化学习 (RL) 优化**：将奖励模型作为环境的“裁判”，使用强化学习算法（如 PPO）来微调 SFT 模型。模型（即 RL 中的“智能体”或“策略”）会不断生成回答，奖励模型会为其打分，RL 算法则根据分数来更新模型的参数，目标是最大化奖励模型的总得分。

在这个框架中，PPO 长期以来都是第三阶段的绝对主力。然而，随着技术的发展，DPO 和 GRPO 提供了新的、可能更高效的路径。

## PPO（近端策略优化）

### 核心思想：在更新策略时不偏离旧策略过远，让训练稳定可靠

### 基本思路：

1. 使用旧策略 actor_old 采样数据

    - Actor_old 根据状态 $ s $ 输出动作分布的  $\mu(s),\; \sigma(s)$

    - 从 Normal 分布采样动作 $ a $

    - 计算旧策略下的 log 概率  $\log \pi_{\text{old}}(a\mid s)$

    - 与环境交互，得到  $r_t,\; done_t$

    - 记录：$(s_t, a_t, \log p_{\text{old}}, r_t, done_t)$

2.  Critic 计算 Value，并用 GAE 得到 Advantage

    - 对 batch 中每个状态，通过 Critic 得到：$V(s_t)$

    - 使用 GAE 计算时序差分误差（TD error）：$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$

    - 计算优势：$A_t = \delta_t + \gamma \lambda A_{t+1}$ (状态 $s$ 下采取 $a$ 优/劣于baseline)

    - 计算 Critic 的回报目标：$return_t = A_t + V(s_t)$

3. 策略更新
    - 新策略 Actor 重新计算动作概率：$\log \pi(a|s)$

    - 计算概率比：$ratio = \frac{\pi(a|s)}{\pi_{old}(a|s)} = e^{\log p - \log p_{\text{old}}}$

    - 代入 PPO-Clip 策略损失：$L^{CLIP} = -\mathbb{E}\left[\min(ratio \cdot A,\; clip(ratio, 1-\epsilon, 1+\epsilon)\cdot A)\right]$

    - 通过梯度下降更新 Actor

4. 价值更新
    - $L_{critic} = (V(s) - return)^2$

    - 优化 MSE Loss 来更新 Critic。

5. 同步策略：更新结束后，将当前策略复制为旧策略



### 在 RLHF 流程中，PPO 的工作流程如下：

1. 初始化：用 SFT 模型的权重初始化策略模型（Policy Model），并通常也用它来初始化价值模型（Value Model）。

2. 采样：从一个指令数据集中随机抽取一个指令（Prompt）。

3. 生成：策略模型根据指令生成一个回答。

4. 评估：奖励模型（RM）对“指令-回答”对打分，得到奖励（Reward）。价值模型（Value Model）对指令进行评估，得到价值（Value）。

5. 计算优势：根据奖励和价值计算优势函数。

6. 更新：使用 PPO 的 Clipped Surrogate Objective 计算损失，并更新策略模型和价值模型的参数。

循环：重复步骤 2-6，直到模型收敛。

同时，为了防止模型在优化过程中“忘记”SFT 阶段学到的知识，或者为了防止模型生成一些虽然奖励高但内容乱七八糟的文本，PPO 的损失函数中通常还会加入一个 KL 散度惩罚项。这个惩罚项用来衡量当前策略与初始 SFT 策略的差异，差异越大，惩罚越重，确保模型在追求高奖励的同时，不会偏离其原始的语言能力。

## DPO（直接偏好优化）

传统 RLHF 是一个“两步走”的过程：先用偏好数据（A 比 B 好）训练一个能给绝对分数（A 得 90 分，B 得 60 分）的奖励模型，然后再用这个分数去指导强化学习。

DPO 的提出者反思道：我们最终的目标不就是让模型知道“A 比 B 好”吗？为什么非要先把它变成“A=90分，B=60分”，再回头去学习这个偏好呢？这个中间的奖励建模步骤不仅复杂，还可能引入误差。我们能不能直接建立一个从“偏好”到“策略更新”的数学桥梁？

DPO 通过直接优化“更好答案 vs 更差答案”的概率差，使得模型学习人类偏好本质上等价于一个二分类问题，而不需要奖励模型和强化学习。

DPO 的工作流程

1. 准备数据：收集偏好数据集，每条数据是（指令，胜出回答，失败回答）的形式。

2. 初始化：加载 SFT 模型作为训练的初始模型 ，并复制一份作为参考模型（在训练中其参数被冻结）。

3. 训练：使用上述 DPO 损失函数进行训练。在每个训练步骤中：
    - 取一个批次（batch）的偏好数据。分别计算模型$\pi_\theta$和$\pi_{reference}$ 在指令下，生成胜出回答和失败回答的概率
    - 代入 DPO 损失函数公式，计算损失。
    - 反向传播，更新$\pi_\theta$的参数。

4. 完成：训练结束后，$\pi_\theta$就是对齐好的模型

整个过程不需要训练一个独立的奖励模型，也不需要复杂的采样和优势计算，更没有价值模型。这使得 DPO 在实现上更简单，训练更稳定，也更节省计算资源。

## GRPO（组别相对策略优化）

DPO 强依赖于高质量的成对偏好数据。如果偏好数据的质量不高，或者标注不一致，DPO 的效果可能会受到影响。

PPO中的优势函数 ，需要一个奖励模型$R_(s,a)$来提供奖励，还需要一个价值模型(Critic)来提供Baseline

利用群体智慧来估计这个$V_s$，要评价一个学生这次考试的成绩（某个回答的奖励）是好是坏（优势），我们不需要知道他历史上的平均分（价值模型的输出），我们可以直接看他这次在班级里（一组回答中）的排名。如果他的分数远高于班级平均分，那么他的优势就是正的，反之亦然。


