---
title: "OpenVLA 笔记"
date: 2024-08-25 10:00:00 +0800
categories: [VLA]
layout: single
author_profile: true
---

# VLA基础知识与OpenVLA

## Transformer
$Attention (Q,K,V) = softmax \frac{QK^T}{\sqrt{dk}}$

其中：

- Q = Query（当前 token）
- K = Key（其他 token 的索引）
- V = Value（其他 token 的内容）

### Encoder 与 Decoder

- Encoder-only	理解任务	BERT

- Decoder-only	生成任务	GPT, LLaMA

- Encoder-Decoder	翻译、摘要	T5, BART


## LLM
大规模自回归 Transformer 模型

通过预测下一个 token 学习语言、逻辑与世界知识

### Llama

Llama：Meta ，Decoder-only Transformer

```
Input tokens → Embedding + RoPE
     ↓
[Attention + FFN] × N 层
     ↓
Linear Projection → Softmax → Next Token
```



<img src="https://pic2.zhimg.com/v2-6d18fd84363055c3668ba4be280d0eb5_r.jpg" width="200" height="550">



一些解释： <br><br>         
1. Input Embedding  
<br>把离散 token（词 ID）映射成连续向量
输出维度是 $[B, T, d_{model}]$
	​
<br><br>


2. RMSNorm
<br><br>Root Mean Square Normalization
LLaMA 用 RMSNorm 替代 LayerNorm：
<br><br>

3. self-attention
<br><br>从输入$x$ 得到三个矩阵 $Q=xW_Q$ , $K=xW_K$,  $V=xW_V$ , $[B, T, d_{model}]$ , 多头$[B, H, T, d_h]$
<br><br>
RoPE（Rotary Position Embedding）
在 Q/K 向量中加入“旋转式相对位置编码”，使模型能感知顺序。
<br><br>
注意力分数 $scores=\frac{QK^T}{\sqrt{dk}}$ $[B, H, T, T]$ (“第 i 个 token 对第 j 个 token 的相关性”)<br><br>causal mask 是一个 $[T, T]$ 的上三角 mask，加到 scores 上，使未来的 token 权重变为 -∞<br><br>Softmax：对最后一个维度（即每一行）做 softmax，每个 token 的注意力权重之和为 1<br><br>加权求和得到上下文向量 $Z=A×V$ → $[B, H, T, d_h]$
每个 token 聚合历史信息<br><br>拼接所有头，过线性层：$[B,T,H×d_h​]$ → $[B, T, d_{model}]$<br><br>把 attention 输出加回原输入，残差连接，即完成了图中下半部分

<br><br>

对于上半部分（FFN）：<br><br>
原始的FFN：$FFN(x)=ReLU(xW1​+b1​)W2​+b2​$, 即先升维后降维

在Llama中，$FFN(x)=(SiLU(xWgate​)⊙(xWup​))Wdown​$ 

其中：

$Wup​$ :  $[d_{model}, d_{ff}]$ , 升维

$Wgate$ : $[d_{model}, d_{ff}]$ ， 激活门控升维

$Wdown$ : $[d_{ff}, d_{model}]$ ， 降维
> $d{ff}$通常是 
$4×𝑑model/3$

>升维 → 降维
让模型在高维空间做更复杂的非线性映射（相当于中间有一个“思考空间”）。

>门控 (GLU)
提供“选择性通道”，能让网络学习哪些特征要增强/抑制。
	​



<br><br>

### 微调
大语言模型本身只是“会续写文本”的预测机。
要让它更符合人类意图，需要后训练 (post-training)：

- SFT (Supervised Fine-Tuning)：在指令/对话数据上微调	让模型“懂指令”
- LoRA / OFT	参数高效微调	小任务上适配
- RLHF (Reinforcement Learning from Human Feedback)	通过奖励模型调整输出风格	让模型“更像人”


## VLM

### 区别于LLM
视觉与语言的cross attention

```
[图像] → Vision Encoder (ViT / CLIP / SigLIP)
              ↓
        投影层 (Projection)
              ↓
     [文本] → LLM (e.g., LLaMA)
```

假设：vision encoder输出 $[B×Nv​×d{vision}]​$

经过映射层 $[B×Nv​×d{model}]$ 后与language encoder拼接 $X=[v1​,v2​,...,v{Nv}​​,t1​,...,t{Nt​}​]$



## VLA(Vision Language Action)

### OpenVLA

> https://arxiv.org/abs/2406.09246  
> https://openvla.github.io

### 基本介绍

OpenVLA 模型规模为 **70 亿参数（7B）**，以 **Llama 2 7B** 作为语言基座。  

视觉编码器采用两个强大的预训练模型 DINOv2 和 SigLIP，以结合空间与语义特征。  

训练数据来自 **Open-X-Embodiment 数据集**（约 97 万条机器人示范），支持多种机器人和任务场景。  


### 模型结构

OpenVLA 的总体结构如下：

**视觉编码器：**  

- DINOv2 提取低层空间特征（如纹理、边缘、形状），  
- SigLIP 提取高级语义特征（如对象类别、颜色、语义关系）。  

两者的输出在通道维度拼接后输入到多模态 Transformer 中。  
这种双分支视觉结构使模型既具备局部几何感知，又具备语义理解能力。

**语言编码器：**  

采用 Llama 2 7B 作为语言处理主干，输入为自然语言任务描述。  
文本被分词后嵌入为 token，与视觉 token 一起输入融合层。  

语言部分主要起到“条件指导”作用（conditioning），引导模型生成与指令对应的动作。  

**动作建模：**  

- 模型将机器人的动作表示为“变化量”（Δ形式），即上一时刻到下一时刻的差分。  
- 每个动作维度（如平移、旋转、夹爪开合）被离散化为若干区间。  
训练时，模型预测下一个动作 token（索引值），推理时再将 token 解码为连续控制信号。  

这种做法让动作预测问题变成了一个“语言式自回归生成问题”。


### 语言推理与局限性

论文指出，语言输入主要作为**控制条件信号**，而不是推理线索。  

由于在多模态模仿训练中，Llama 2 的原始语言能力（推理，解释）逐渐被覆盖，模型更像是“视觉-语言条件控制器”，而非真正具备推理能力的智能体。  

这种设计使模型结构简单，但限制了它在复杂语言规划任务中的表现。



### OpenVLA-oft

OpenVLA 团队还推出了 **OFT（Optimized Fine-Tuning）**，  
用于在特定机器人或任务上进行高效微调。  

该方法基于 **LoRA（Low-Rank Adaptation）**，  
在 Transformer 的权重矩阵（$W_k$、$W_v$、$W_o$、$W_q$）上添加低秩可学习偏移项：  

$W' = W + \Delta W$


这种方式能显著降低显存占用与训练成本，并允许在嵌入式硬件（如 Jetson）上快速部署。



