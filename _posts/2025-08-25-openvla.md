---
title: "OpenVLA 笔记"
date: 2025-08-25 10:00:00 +0800
categories: [VLA]
layout: single
author_profile: true
---

# 大模型

## Transformer
$$

 Attention (Q,K,V) = softmax \frac{QK^T}{\sqrt{dk}}

$$
其中：

- Q = Query（当前 token）
- K = Key（其他 token 的索引）
- V = Value（其他 token 的内容）

### Encoder 与 Decoder
Encoder-only	理解任务	BERT

Decoder-only	生成任务	GPT, LLaMA

Encoder-Decoder	翻译、摘要	T5, BART

### Decoder-only的 transformer 层


## Mamba


## LLM
大规模自回归 Transformer 模型

通过预测下一个 token 学习语言、逻辑与世界知识

### Llama

Llama：Meta ，Decoder-only Transformer

```
Input tokens → Embedding + RoPE
     ↓
[Attention + FFN] × N 层
     ↓
Linear Projection → Softmax → Next Token
```



<img src="https://pic2.zhimg.com/v2-6d18fd84363055c3668ba4be280d0eb5_r.jpg" width="200" height="550">



一些解释： <br><br>         
1. Input Embedding  
<br>把离散 token（词 ID）映射成连续向量
输出维度是 $[B, T, d_{model}]$
	​
<br><br>


2. RMSNorm
<br><br>Root Mean Square Normalization
LLaMA 用 RMSNorm 替代 LayerNorm：
<br><br>

3. self-attention
<br><br>从输入$x$ 得到三个矩阵 $Q=xW_Q$ , $K=xW_K$,  $V=xW_V$ , $[B, T, d_{model}]$ , 多头$[B, H, T, d_h]$
<br><br>
RoPE（Rotary Position Embedding）
在 Q/K 向量中加入“旋转式相对位置编码”，使模型能感知顺序。
<br><br>
注意力分数 $scores=\frac{QK^T}{\sqrt{dk}}$ $[B, H, T, T]$ (“第 i 个 token 对第 j 个 token 的相关性”)<br><br>causal mask 是一个 $[T, T]$ 的上三角 mask，加到 scores 上，使未来的 token 权重变为 -∞<br><br>Softmax：对最后一个维度（即每一行）做 softmax，每个 token 的注意力权重之和为 1<br><br>加权求和得到上下文向量 $Z=A×V$ → $[B, H, T, d_h]$
每个 token 聚合历史信息<br><br>拼接所有头，过线性层：$[B,T,H×d_h​]$ → $[B, T, d_{model}]$<br><br>把 attention 输出加回原输入，残差连接，即完成了图中下半部分

<br><br>

对于上半部分（FFN）：<br><br>
原始的FFN：$FFN(x)=ReLU(xW1​+b1​)W2​+b2​$, 即先升维后降维

在Llama中，$FFN(x)=(SiLU(xWgate​)⊙(xWup​))Wdown​$ 

其中：

$Wup​$ :  $[d_{model}, d_{ff}]$ , 升维

$Wgate$ : $[d_{model}, d_{ff}]$ ， 激活门控升维

$Wdown$ : $[d_{ff}, d_{model}]$ ， 降维
> $d{ff}$通常是 
$4×𝑑model/3$

>升维 → 降维
让模型在高维空间做更复杂的非线性映射（相当于中间有一个“思考空间”）。

>门控 (GLU)
提供“选择性通道”，能让网络学习哪些特征要增强/抑制。
	​



<br><br>

### 微调
大语言模型本身只是“会续写文本”的预测机。
要让它更符合人类意图，需要后训练 (post-training)：

- SFT (Supervised Fine-Tuning)：在指令/对话数据上微调	让模型“懂指令”
- LoRA / OFT	参数高效微调	小任务上适配
- RLHF (Reinforcement Learning from Human Feedback)	通过奖励模型调整输出风格	让模型“更像人”


## VLM

### 区别于LLM
视觉与语言的cross attention

```
[图像] → Vision Encoder (ViT / CLIP / SigLIP)
              ↓
        投影层 (Projection)
              ↓
     [文本] → LLM (e.g., LLaMA)
```

假设：vision encoder输出 $[B×Nv​×d{vision}]​$

经过映射层 $[B×Nv​×d{model}]$ 后与language encoder拼接 $X=[v1​,v2​,...,v{Nv}​​,t1​,...,t{Nt​}​]$



## VLA
vision language action

### OpenVLA
openvla：Llama 2 7B

Llama 分词器 +prompt

token 

action：delta

- 预训练的vlm模型：prismatic -7B :

  - llama tokenizer 语言编码器

  - dinov2 和 siglip分别处理低级空间特征（纹理，位置，形状）和高级语义特征（类别，颜色） 按通道维度进行拼接

- llama 理解输入的视觉和语言特征 生成动作序列

与历史动作 token 一起输入后续 Transformer 层
        ↓
预测下一个动作 token
        ↓
解码为连续控制信号

动作序列解码器：输入：128*128+语言指令
输出：一组含有终止信号 机器人末端执行器的平移 旋转 夹爪开合程度


将每个动作维度的参数变化范围限定在[P1,P99] 百分位 -->离散的索引


解码：返回各个索引中间值

#### 数据
Open-X-Embodiment 进行一定筛选（单臂）

#### 实验
- 视觉泛化 
- 运动泛化 
- 物理泛化

无CoT，原因在于OpenVLA中语言 token 只起到 conditioning 作用，LLaMA 的语言能力（包括 CoT、推理、解释）在这种训练下逐渐被覆盖或“遗忘”


### OpenVLA-oft
Lora
对Wk，Wv，Wo进行W+deita



